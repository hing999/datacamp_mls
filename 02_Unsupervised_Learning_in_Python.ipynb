{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMB612/ps0BC+H6hH5y0L5b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hing999/datacamp_mls/blob/main/02_Unsupervised_Learning_in_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qrnnRD6kbIh8"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iris"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YELkH9mWeC0u",
        "outputId": "2c6cf0c0-1ce7-4ed0-b6fb-af36b5db1f3f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
              "        [4.9, 3. , 1.4, 0.2],\n",
              "        [4.7, 3.2, 1.3, 0.2],\n",
              "        [4.6, 3.1, 1.5, 0.2],\n",
              "        [5. , 3.6, 1.4, 0.2],\n",
              "        [5.4, 3.9, 1.7, 0.4],\n",
              "        [4.6, 3.4, 1.4, 0.3],\n",
              "        [5. , 3.4, 1.5, 0.2],\n",
              "        [4.4, 2.9, 1.4, 0.2],\n",
              "        [4.9, 3.1, 1.5, 0.1],\n",
              "        [5.4, 3.7, 1.5, 0.2],\n",
              "        [4.8, 3.4, 1.6, 0.2],\n",
              "        [4.8, 3. , 1.4, 0.1],\n",
              "        [4.3, 3. , 1.1, 0.1],\n",
              "        [5.8, 4. , 1.2, 0.2],\n",
              "        [5.7, 4.4, 1.5, 0.4],\n",
              "        [5.4, 3.9, 1.3, 0.4],\n",
              "        [5.1, 3.5, 1.4, 0.3],\n",
              "        [5.7, 3.8, 1.7, 0.3],\n",
              "        [5.1, 3.8, 1.5, 0.3],\n",
              "        [5.4, 3.4, 1.7, 0.2],\n",
              "        [5.1, 3.7, 1.5, 0.4],\n",
              "        [4.6, 3.6, 1. , 0.2],\n",
              "        [5.1, 3.3, 1.7, 0.5],\n",
              "        [4.8, 3.4, 1.9, 0.2],\n",
              "        [5. , 3. , 1.6, 0.2],\n",
              "        [5. , 3.4, 1.6, 0.4],\n",
              "        [5.2, 3.5, 1.5, 0.2],\n",
              "        [5.2, 3.4, 1.4, 0.2],\n",
              "        [4.7, 3.2, 1.6, 0.2],\n",
              "        [4.8, 3.1, 1.6, 0.2],\n",
              "        [5.4, 3.4, 1.5, 0.4],\n",
              "        [5.2, 4.1, 1.5, 0.1],\n",
              "        [5.5, 4.2, 1.4, 0.2],\n",
              "        [4.9, 3.1, 1.5, 0.2],\n",
              "        [5. , 3.2, 1.2, 0.2],\n",
              "        [5.5, 3.5, 1.3, 0.2],\n",
              "        [4.9, 3.6, 1.4, 0.1],\n",
              "        [4.4, 3. , 1.3, 0.2],\n",
              "        [5.1, 3.4, 1.5, 0.2],\n",
              "        [5. , 3.5, 1.3, 0.3],\n",
              "        [4.5, 2.3, 1.3, 0.3],\n",
              "        [4.4, 3.2, 1.3, 0.2],\n",
              "        [5. , 3.5, 1.6, 0.6],\n",
              "        [5.1, 3.8, 1.9, 0.4],\n",
              "        [4.8, 3. , 1.4, 0.3],\n",
              "        [5.1, 3.8, 1.6, 0.2],\n",
              "        [4.6, 3.2, 1.4, 0.2],\n",
              "        [5.3, 3.7, 1.5, 0.2],\n",
              "        [5. , 3.3, 1.4, 0.2],\n",
              "        [7. , 3.2, 4.7, 1.4],\n",
              "        [6.4, 3.2, 4.5, 1.5],\n",
              "        [6.9, 3.1, 4.9, 1.5],\n",
              "        [5.5, 2.3, 4. , 1.3],\n",
              "        [6.5, 2.8, 4.6, 1.5],\n",
              "        [5.7, 2.8, 4.5, 1.3],\n",
              "        [6.3, 3.3, 4.7, 1.6],\n",
              "        [4.9, 2.4, 3.3, 1. ],\n",
              "        [6.6, 2.9, 4.6, 1.3],\n",
              "        [5.2, 2.7, 3.9, 1.4],\n",
              "        [5. , 2. , 3.5, 1. ],\n",
              "        [5.9, 3. , 4.2, 1.5],\n",
              "        [6. , 2.2, 4. , 1. ],\n",
              "        [6.1, 2.9, 4.7, 1.4],\n",
              "        [5.6, 2.9, 3.6, 1.3],\n",
              "        [6.7, 3.1, 4.4, 1.4],\n",
              "        [5.6, 3. , 4.5, 1.5],\n",
              "        [5.8, 2.7, 4.1, 1. ],\n",
              "        [6.2, 2.2, 4.5, 1.5],\n",
              "        [5.6, 2.5, 3.9, 1.1],\n",
              "        [5.9, 3.2, 4.8, 1.8],\n",
              "        [6.1, 2.8, 4. , 1.3],\n",
              "        [6.3, 2.5, 4.9, 1.5],\n",
              "        [6.1, 2.8, 4.7, 1.2],\n",
              "        [6.4, 2.9, 4.3, 1.3],\n",
              "        [6.6, 3. , 4.4, 1.4],\n",
              "        [6.8, 2.8, 4.8, 1.4],\n",
              "        [6.7, 3. , 5. , 1.7],\n",
              "        [6. , 2.9, 4.5, 1.5],\n",
              "        [5.7, 2.6, 3.5, 1. ],\n",
              "        [5.5, 2.4, 3.8, 1.1],\n",
              "        [5.5, 2.4, 3.7, 1. ],\n",
              "        [5.8, 2.7, 3.9, 1.2],\n",
              "        [6. , 2.7, 5.1, 1.6],\n",
              "        [5.4, 3. , 4.5, 1.5],\n",
              "        [6. , 3.4, 4.5, 1.6],\n",
              "        [6.7, 3.1, 4.7, 1.5],\n",
              "        [6.3, 2.3, 4.4, 1.3],\n",
              "        [5.6, 3. , 4.1, 1.3],\n",
              "        [5.5, 2.5, 4. , 1.3],\n",
              "        [5.5, 2.6, 4.4, 1.2],\n",
              "        [6.1, 3. , 4.6, 1.4],\n",
              "        [5.8, 2.6, 4. , 1.2],\n",
              "        [5. , 2.3, 3.3, 1. ],\n",
              "        [5.6, 2.7, 4.2, 1.3],\n",
              "        [5.7, 3. , 4.2, 1.2],\n",
              "        [5.7, 2.9, 4.2, 1.3],\n",
              "        [6.2, 2.9, 4.3, 1.3],\n",
              "        [5.1, 2.5, 3. , 1.1],\n",
              "        [5.7, 2.8, 4.1, 1.3],\n",
              "        [6.3, 3.3, 6. , 2.5],\n",
              "        [5.8, 2.7, 5.1, 1.9],\n",
              "        [7.1, 3. , 5.9, 2.1],\n",
              "        [6.3, 2.9, 5.6, 1.8],\n",
              "        [6.5, 3. , 5.8, 2.2],\n",
              "        [7.6, 3. , 6.6, 2.1],\n",
              "        [4.9, 2.5, 4.5, 1.7],\n",
              "        [7.3, 2.9, 6.3, 1.8],\n",
              "        [6.7, 2.5, 5.8, 1.8],\n",
              "        [7.2, 3.6, 6.1, 2.5],\n",
              "        [6.5, 3.2, 5.1, 2. ],\n",
              "        [6.4, 2.7, 5.3, 1.9],\n",
              "        [6.8, 3. , 5.5, 2.1],\n",
              "        [5.7, 2.5, 5. , 2. ],\n",
              "        [5.8, 2.8, 5.1, 2.4],\n",
              "        [6.4, 3.2, 5.3, 2.3],\n",
              "        [6.5, 3. , 5.5, 1.8],\n",
              "        [7.7, 3.8, 6.7, 2.2],\n",
              "        [7.7, 2.6, 6.9, 2.3],\n",
              "        [6. , 2.2, 5. , 1.5],\n",
              "        [6.9, 3.2, 5.7, 2.3],\n",
              "        [5.6, 2.8, 4.9, 2. ],\n",
              "        [7.7, 2.8, 6.7, 2. ],\n",
              "        [6.3, 2.7, 4.9, 1.8],\n",
              "        [6.7, 3.3, 5.7, 2.1],\n",
              "        [7.2, 3.2, 6. , 1.8],\n",
              "        [6.2, 2.8, 4.8, 1.8],\n",
              "        [6.1, 3. , 4.9, 1.8],\n",
              "        [6.4, 2.8, 5.6, 2.1],\n",
              "        [7.2, 3. , 5.8, 1.6],\n",
              "        [7.4, 2.8, 6.1, 1.9],\n",
              "        [7.9, 3.8, 6.4, 2. ],\n",
              "        [6.4, 2.8, 5.6, 2.2],\n",
              "        [6.3, 2.8, 5.1, 1.5],\n",
              "        [6.1, 2.6, 5.6, 1.4],\n",
              "        [7.7, 3. , 6.1, 2.3],\n",
              "        [6.3, 3.4, 5.6, 2.4],\n",
              "        [6.4, 3.1, 5.5, 1.8],\n",
              "        [6. , 3. , 4.8, 1.8],\n",
              "        [6.9, 3.1, 5.4, 2.1],\n",
              "        [6.7, 3.1, 5.6, 2.4],\n",
              "        [6.9, 3.1, 5.1, 2.3],\n",
              "        [5.8, 2.7, 5.1, 1.9],\n",
              "        [6.8, 3.2, 5.9, 2.3],\n",
              "        [6.7, 3.3, 5.7, 2.5],\n",
              "        [6.7, 3. , 5.2, 2.3],\n",
              "        [6.3, 2.5, 5. , 1.9],\n",
              "        [6.5, 3. , 5.2, 2. ],\n",
              "        [6.2, 3.4, 5.4, 2.3],\n",
              "        [5.9, 3. , 5.1, 1.8]]),\n",
              " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
              " 'frame': None,\n",
              " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
              " 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...',\n",
              " 'feature_names': ['sepal length (cm)',\n",
              "  'sepal width (cm)',\n",
              "  'petal length (cm)',\n",
              "  'petal width (cm)'],\n",
              " 'filename': 'iris.csv',\n",
              " 'data_module': 'sklearn.datasets.data'}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(species)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "LY03uBHrePxf",
        "outputId": "87fe6ef4-4aad-4458-be88-0de821b89ab9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-93e2d6e63be0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspecies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'species' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering for dataset exploratio"
      ],
      "metadata": {
        "id": "eDFQ8JRbbTEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "samples = [[ 5., 3.3, 1.4, 0.2]\n",
        ",[ 5. ,3.5 ,1.3 ,0.3]\n",
        ",[ 7.2 ,3.2 ,6. ,1.8]]"
      ],
      "metadata": {
        "id": "a32TtMG8cQPH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hy8xJtOBbTuX",
        "outputId": "512f8953-4323-4162-cbc3-b4d9a7e451ed"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[5.0, 3.3, 1.4, 0.2], [5.0, 3.5, 1.3, 0.3], [7.2, 3.2, 6.0, 1.8]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "model = KMeans(n_clusters=3)\n",
        "model.fit(samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "060R6S2pcOQR",
        "outputId": "1714a110-2510-4fdf-b6d9-2f72a1725bf9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KMeans(n_clusters=3)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = model.predict(samples)\n",
        "print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JKW5q1pccu7",
        "outputId": "8e84f495-3103-4529-fa65-79cee5719326"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 2 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_samples = samples\n",
        "\n",
        "new_labels = model.predict(new_samples)\n",
        "print(new_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWCnLs00cfXp",
        "outputId": "cb0fd2c9-c2ef-4c42-83c2-04c45c7c3ae2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 2 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatter plots\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "xs = samples[:,0]\n",
        "ys = samples[:,2]\n",
        "plt.scatter(xs, ys, c=labels)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "OVwhkcMGcoSS",
        "outputId": "4e09dbde-6893-471e-c59c-560df9633583"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-afc86302784f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cMXBH03ShjaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise_Clustering 2D points\n",
        "\n",
        "# Import KMeans\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Create a KMeans instance with 3 clusters: model\n",
        "model = KMeans(n_clusters= 3)\n",
        "\n",
        "# Fit model to points\n",
        "model.fit(points)\n",
        "\n",
        "# Determine the cluster labels of new_points: labels\n",
        "labels = model.predict(new_points)\n",
        "\n",
        "# Print cluster labels of new_points\n",
        "print(labels)\n"
      ],
      "metadata": {
        "id": "SeaC67IRdbrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise_Inspect your clustering\n",
        "\n",
        "# Import pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assign the columns of new_points: xs and ys\n",
        "xs = new_points[:,0]\n",
        "ys = new_points[:,1]\n",
        "\n",
        "# Make a scatter plot of xs and ys, using labels to define the colors\n",
        "plt.scatter(xs, ys, c=labels, alpha=0.5)\n",
        "\n",
        "# Assign the cluster centers: centroids\n",
        "centroids = model.cluster_centers_\n",
        "\n",
        "\n",
        "# Assign the columns of centroids: centroids_x, centroids_y\n",
        "centroids_x = centroids[:,0]\n",
        "centroids_y = centroids[:,1]\n",
        "\n",
        "# Make a scatter plot of centroids_x and centroids_y\n",
        "plt.scatter(centroids_x, centroids_y, marker = \"D\", s= 50)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "9CbC3hsmhnPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris[\"data\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xl1Wma1teBBB",
        "outputId": "677bc0d5-9d8c-4dee-815b-6ccddb9b3712"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.1, 3.5, 1.4, 0.2],\n",
              "       [4.9, 3. , 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.3, 0.2],\n",
              "       [4.6, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.6, 1.4, 0.2],\n",
              "       [5.4, 3.9, 1.7, 0.4],\n",
              "       [4.6, 3.4, 1.4, 0.3],\n",
              "       [5. , 3.4, 1.5, 0.2],\n",
              "       [4.4, 2.9, 1.4, 0.2],\n",
              "       [4.9, 3.1, 1.5, 0.1],\n",
              "       [5.4, 3.7, 1.5, 0.2],\n",
              "       [4.8, 3.4, 1.6, 0.2],\n",
              "       [4.8, 3. , 1.4, 0.1],\n",
              "       [4.3, 3. , 1.1, 0.1],\n",
              "       [5.8, 4. , 1.2, 0.2],\n",
              "       [5.7, 4.4, 1.5, 0.4],\n",
              "       [5.4, 3.9, 1.3, 0.4],\n",
              "       [5.1, 3.5, 1.4, 0.3],\n",
              "       [5.7, 3.8, 1.7, 0.3],\n",
              "       [5.1, 3.8, 1.5, 0.3],\n",
              "       [5.4, 3.4, 1.7, 0.2],\n",
              "       [5.1, 3.7, 1.5, 0.4],\n",
              "       [4.6, 3.6, 1. , 0.2],\n",
              "       [5.1, 3.3, 1.7, 0.5],\n",
              "       [4.8, 3.4, 1.9, 0.2],\n",
              "       [5. , 3. , 1.6, 0.2],\n",
              "       [5. , 3.4, 1.6, 0.4],\n",
              "       [5.2, 3.5, 1.5, 0.2],\n",
              "       [5.2, 3.4, 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.6, 0.2],\n",
              "       [4.8, 3.1, 1.6, 0.2],\n",
              "       [5.4, 3.4, 1.5, 0.4],\n",
              "       [5.2, 4.1, 1.5, 0.1],\n",
              "       [5.5, 4.2, 1.4, 0.2],\n",
              "       [4.9, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.2, 1.2, 0.2],\n",
              "       [5.5, 3.5, 1.3, 0.2],\n",
              "       [4.9, 3.6, 1.4, 0.1],\n",
              "       [4.4, 3. , 1.3, 0.2],\n",
              "       [5.1, 3.4, 1.5, 0.2],\n",
              "       [5. , 3.5, 1.3, 0.3],\n",
              "       [4.5, 2.3, 1.3, 0.3],\n",
              "       [4.4, 3.2, 1.3, 0.2],\n",
              "       [5. , 3.5, 1.6, 0.6],\n",
              "       [5.1, 3.8, 1.9, 0.4],\n",
              "       [4.8, 3. , 1.4, 0.3],\n",
              "       [5.1, 3.8, 1.6, 0.2],\n",
              "       [4.6, 3.2, 1.4, 0.2],\n",
              "       [5.3, 3.7, 1.5, 0.2],\n",
              "       [5. , 3.3, 1.4, 0.2],\n",
              "       [7. , 3.2, 4.7, 1.4],\n",
              "       [6.4, 3.2, 4.5, 1.5],\n",
              "       [6.9, 3.1, 4.9, 1.5],\n",
              "       [5.5, 2.3, 4. , 1.3],\n",
              "       [6.5, 2.8, 4.6, 1.5],\n",
              "       [5.7, 2.8, 4.5, 1.3],\n",
              "       [6.3, 3.3, 4.7, 1.6],\n",
              "       [4.9, 2.4, 3.3, 1. ],\n",
              "       [6.6, 2.9, 4.6, 1.3],\n",
              "       [5.2, 2.7, 3.9, 1.4],\n",
              "       [5. , 2. , 3.5, 1. ],\n",
              "       [5.9, 3. , 4.2, 1.5],\n",
              "       [6. , 2.2, 4. , 1. ],\n",
              "       [6.1, 2.9, 4.7, 1.4],\n",
              "       [5.6, 2.9, 3.6, 1.3],\n",
              "       [6.7, 3.1, 4.4, 1.4],\n",
              "       [5.6, 3. , 4.5, 1.5],\n",
              "       [5.8, 2.7, 4.1, 1. ],\n",
              "       [6.2, 2.2, 4.5, 1.5],\n",
              "       [5.6, 2.5, 3.9, 1.1],\n",
              "       [5.9, 3.2, 4.8, 1.8],\n",
              "       [6.1, 2.8, 4. , 1.3],\n",
              "       [6.3, 2.5, 4.9, 1.5],\n",
              "       [6.1, 2.8, 4.7, 1.2],\n",
              "       [6.4, 2.9, 4.3, 1.3],\n",
              "       [6.6, 3. , 4.4, 1.4],\n",
              "       [6.8, 2.8, 4.8, 1.4],\n",
              "       [6.7, 3. , 5. , 1.7],\n",
              "       [6. , 2.9, 4.5, 1.5],\n",
              "       [5.7, 2.6, 3.5, 1. ],\n",
              "       [5.5, 2.4, 3.8, 1.1],\n",
              "       [5.5, 2.4, 3.7, 1. ],\n",
              "       [5.8, 2.7, 3.9, 1.2],\n",
              "       [6. , 2.7, 5.1, 1.6],\n",
              "       [5.4, 3. , 4.5, 1.5],\n",
              "       [6. , 3.4, 4.5, 1.6],\n",
              "       [6.7, 3.1, 4.7, 1.5],\n",
              "       [6.3, 2.3, 4.4, 1.3],\n",
              "       [5.6, 3. , 4.1, 1.3],\n",
              "       [5.5, 2.5, 4. , 1.3],\n",
              "       [5.5, 2.6, 4.4, 1.2],\n",
              "       [6.1, 3. , 4.6, 1.4],\n",
              "       [5.8, 2.6, 4. , 1.2],\n",
              "       [5. , 2.3, 3.3, 1. ],\n",
              "       [5.6, 2.7, 4.2, 1.3],\n",
              "       [5.7, 3. , 4.2, 1.2],\n",
              "       [5.7, 2.9, 4.2, 1.3],\n",
              "       [6.2, 2.9, 4.3, 1.3],\n",
              "       [5.1, 2.5, 3. , 1.1],\n",
              "       [5.7, 2.8, 4.1, 1.3],\n",
              "       [6.3, 3.3, 6. , 2.5],\n",
              "       [5.8, 2.7, 5.1, 1.9],\n",
              "       [7.1, 3. , 5.9, 2.1],\n",
              "       [6.3, 2.9, 5.6, 1.8],\n",
              "       [6.5, 3. , 5.8, 2.2],\n",
              "       [7.6, 3. , 6.6, 2.1],\n",
              "       [4.9, 2.5, 4.5, 1.7],\n",
              "       [7.3, 2.9, 6.3, 1.8],\n",
              "       [6.7, 2.5, 5.8, 1.8],\n",
              "       [7.2, 3.6, 6.1, 2.5],\n",
              "       [6.5, 3.2, 5.1, 2. ],\n",
              "       [6.4, 2.7, 5.3, 1.9],\n",
              "       [6.8, 3. , 5.5, 2.1],\n",
              "       [5.7, 2.5, 5. , 2. ],\n",
              "       [5.8, 2.8, 5.1, 2.4],\n",
              "       [6.4, 3.2, 5.3, 2.3],\n",
              "       [6.5, 3. , 5.5, 1.8],\n",
              "       [7.7, 3.8, 6.7, 2.2],\n",
              "       [7.7, 2.6, 6.9, 2.3],\n",
              "       [6. , 2.2, 5. , 1.5],\n",
              "       [6.9, 3.2, 5.7, 2.3],\n",
              "       [5.6, 2.8, 4.9, 2. ],\n",
              "       [7.7, 2.8, 6.7, 2. ],\n",
              "       [6.3, 2.7, 4.9, 1.8],\n",
              "       [6.7, 3.3, 5.7, 2.1],\n",
              "       [7.2, 3.2, 6. , 1.8],\n",
              "       [6.2, 2.8, 4.8, 1.8],\n",
              "       [6.1, 3. , 4.9, 1.8],\n",
              "       [6.4, 2.8, 5.6, 2.1],\n",
              "       [7.2, 3. , 5.8, 1.6],\n",
              "       [7.4, 2.8, 6.1, 1.9],\n",
              "       [7.9, 3.8, 6.4, 2. ],\n",
              "       [6.4, 2.8, 5.6, 2.2],\n",
              "       [6.3, 2.8, 5.1, 1.5],\n",
              "       [6.1, 2.6, 5.6, 1.4],\n",
              "       [7.7, 3. , 6.1, 2.3],\n",
              "       [6.3, 3.4, 5.6, 2.4],\n",
              "       [6.4, 3.1, 5.5, 1.8],\n",
              "       [6. , 3. , 4.8, 1.8],\n",
              "       [6.9, 3.1, 5.4, 2.1],\n",
              "       [6.7, 3.1, 5.6, 2.4],\n",
              "       [6.9, 3.1, 5.1, 2.3],\n",
              "       [5.8, 2.7, 5.1, 1.9],\n",
              "       [6.8, 3.2, 5.9, 2.3],\n",
              "       [6.7, 3.3, 5.7, 2.5],\n",
              "       [6.7, 3. , 5.2, 2.3],\n",
              "       [6.3, 2.5, 5. , 1.9],\n",
              "       [6.5, 3. , 5.2, 2. ],\n",
              "       [6.2, 3.4, 5.4, 2.3],\n",
              "       [5.9, 3. , 5.1, 1.8]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aligning labels and species\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame({'labels': labels, 'species': species})\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "hYIErT0Dd8Ag",
        "outputId": "0d6091ab-4e1b-464f-90ba-8591d232971e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-8b2964ddef0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'species'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mspecies\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'species' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Crosstab of labels and species\n",
        "ct = pd.crosstab(df['labels'], df['species'])\n",
        "print(ct)"
      ],
      "metadata": {
        "id": "gO_Do-TwfR1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inertia measures clustering quality\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "model = KMeans(n_clusters=3)\n",
        "model.fit(samples)\n",
        "print(model.inertia_)"
      ],
      "metadata": {
        "id": "28jpPQ4QfXlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercicse_How many clusters of grain?\n",
        "\n",
        "ks = range(1, 6)\n",
        "inertias = []\n",
        "\n",
        "for k in ks:\n",
        "    # Create a KMeans instance with k clusters: model\n",
        "    model = KMeans(n_clusters = k )\n",
        "    \n",
        "    # Fit model to samples\n",
        "    model.fit(samples )\n",
        "    \n",
        "    # Append the inertia to the list of inertias\n",
        "    inertias.append(model.inertia_)\n",
        "    \n",
        "# Plot ks vs inertias\n",
        "plt.plot(ks, inertias, '-o')\n",
        "plt.xlabel('number of clusters, k')\n",
        "plt.ylabel('inertia')\n",
        "plt.xticks(ks)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rYSuK3p7cuSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise_Evaluating the grain clustering\n",
        "\n",
        "# Create a KMeans model with 3 clusters: model\n",
        "model = KMeans(n_clusters = 3 )\n",
        "\n",
        "# Use fit_predict to fit model and obtain cluster labels: labels\n",
        "labels = model.fit_predict(samples)\n",
        "\n",
        "# Create a DataFrame witah labels and varieties as columns: df\n",
        "df = pd.DataFrame({'labels': labels, 'varieties': varieties})\n",
        "\n",
        "# Create crosstab: ct\n",
        "ct = pd.crosstab(df['labels'], df['varieties'])\n",
        "\n",
        "# Display ct\n",
        "print(ct)\n"
      ],
      "metadata": {
        "id": "62JDxLpsjFeM"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ak7LnvZ6jHAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transforming features for better clusterings"
      ],
      "metadata": {
        "id": "yqFqZthdfhvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = sklearn.datasets.load_wine()"
      ],
      "metadata": {
        "id": "_bA96HH4j_Q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clustering the wines\n",
        "from sklearn.cluster import KMeans\n",
        "model = KMeans(n_clusters=3)\n",
        "labels = model.fit_predict(samples)"
      ],
      "metadata": {
        "id": "-gXcyOYOfkRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clusters vs. varieties\n",
        "df = pd.DataFrame({'labels': labels, 'varieties': varieties})\n",
        "ct = pd.crosstab(df['labels'], df['varieties'])\n",
        "print(ct)\n",
        "varieties Barbera"
      ],
      "metadata": {
        "id": "nRyjixjFfo7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sklearn StandardScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(samples)\n",
        "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
        "samples_scaled = scaler.transform(samples)"
      ],
      "metadata": {
        "id": "fC8mxNyefo40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipelines combine multiple steps\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "scaler = StandardScaler()\n",
        "kmeans = KMeans(n_clusters=3)\n",
        "from sklearn.pipeline import make_pipeline\n",
        "pipeline = make_pipeline(scaler, kmeans)\n",
        "pipeline.fit(samples)\n",
        "\n",
        "labels = pipeline.predict(samples)"
      ],
      "metadata": {
        "id": "soXyqZINfo2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "69Pf7Sgrfozb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Excercise\n",
        "# Scaling fish data for clustering\n",
        "\n",
        "# Perform the necessary imports\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Create scaler: scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Create KMeans instance: kmeans\n",
        "kmeans = KMeans(n_clusters=4)\n",
        "\n",
        "# Create pipeline: pipeline\n",
        "pipeline = make_pipeline(scaler, kmeans)\n"
      ],
      "metadata": {
        "id": "LnCg2UTifow1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise\n",
        "# Clustering the fish data\n",
        "\n",
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Fit the pipeline to samples\n",
        "pipeline.fit(samples)\n",
        "\n",
        "# Calculate the cluster labels: labels\n",
        "labels = pipeline.predict(samples)\n",
        "\n",
        "# Create a DataFrame with labels and species as columns: df\n",
        "df = pd.DataFrame({'labels': labels, 'species': species})\n",
        "\n",
        "# Create crosstab: ct\n",
        "ct = pd.crosstab(df['labels'], df['species'])\n",
        "\n",
        "# Display ct\n",
        "print(ct)\n"
      ],
      "metadata": {
        "id": "OfZ19kZDfouO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise\n",
        "# Clustering stocks using KMeans\n",
        "\n",
        "# Import Normalizer\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "# Create a normalizer: normalizer\n",
        "normalizer = Normalizer()\n",
        "\n",
        "# Create a KMeans model with 10 clusters: kmeans\n",
        "kmeans = KMeans(n_clusters=10)\n",
        "\n",
        "# Make a pipeline chaining normalizer and kmeans: pipeline\n",
        "pipeline = make_pipeline(normalizer, kmeans)\n",
        "\n",
        "# Fit pipeline to the daily price movements\n",
        "pipeline.fit(movements)\n"
      ],
      "metadata": {
        "id": "Ip7TkpCYforo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise\n",
        "# Which stocks move together?\n",
        "\n",
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Predict the cluster labels: labels\n",
        "labels = pipeline.predict(movements)\n",
        "\n",
        "# Create a DataFrame aligning labels and companies: df\n",
        "df = pd.DataFrame({'labels': labels, 'companies': companies})\n",
        "\n",
        "# Display df sorted by cluster label\n",
        "print(df.sort_values('labels') )"
      ],
      "metadata": {
        "id": "45ZWnQ38foow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xbYqwsYbfol8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization with hierarchical clustering and t-SNE"
      ],
      "metadata": {
        "id": "FVa4qpjcUjnB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YFbRxjTFUk7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing hierarchies"
      ],
      "metadata": {
        "id": "5-LSg-RcUuae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hierarchical clustering with SciPy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "mergings = linkage(samples, method='complete')\n",
        "dendrogram(mergings,\n",
        "           labels=country_names,\n",
        "           leaf_rotation=90,\n",
        "           leaf_font_size=6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "40TFNU5pUwxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise\n",
        "# Hierarchical clustering of the grain data\n",
        "\n",
        "# Perform the necessary imports\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate the linkage: mergings\n",
        "mergings = linkage(samples, method='complete')\n",
        "\n",
        "# Plot the dendrogram, using varieties as labels\n",
        "dendrogram(mergings,\n",
        "           labels=varieties,\n",
        "           leaf_rotation=90,\n",
        "           leaf_font_size=6\n",
        ")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "raUhKAKlUwu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise\n",
        "# Hierarchies of stocks\n",
        "\n",
        "# Import normalize\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "# Normalize the movements: normalized_movements\n",
        "normalized_movements = normalize(movements)\n",
        "\n",
        "# Calculate the linkage: mergings\n",
        "mergings = linkage(normalized_movements, method='complete')\n",
        "\n",
        "# Plot the dendrogram\n",
        "dendrogram(mergings,\n",
        "           labels=companies,\n",
        "           leaf_rotation=90,\n",
        "           leaf_font_size=6)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o91izDGwWQzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Na-Hgc2LWQYo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cluster labels in hierarchical clustering"
      ],
      "metadata": {
        "id": "Q-Zgvg-3U6Ef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting cluster labels using fcluster\n",
        "\n",
        "from scipy.cluster.hierarchy import linkage\n",
        "mergings = linkage(samples, method='complete')\n",
        "from scipy.cluster.hierarchy import fcluster\n",
        "labels = fcluster(mergings, 15, criterion='distance')\n",
        "print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8eaE6oyUwsh",
        "outputId": "1ceb1e01-4616-43a0-8807-c02420214847"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aligning cluster labels with country names\n",
        "\n",
        "import pandas as pd\n",
        "pairs = pd.DataFrame({'labels': labels, 'countries': country_names})\n",
        "print(pairs.sort_values('labels'))"
      ],
      "metadata": {
        "id": "O7jjRGNqUwp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise\n",
        "# Different linkage, different hierarchical clustering!\n",
        "\n",
        "# Perform the necessary imports\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "\n",
        "# Calculate the linkage: mergings\n",
        "mergings = linkage(samples, method='single')\n",
        "\n",
        "# Plot the dendrogram\n",
        "dendrogram(mergings,\n",
        "           labels=country_names,\n",
        "           leaf_rotation=90,\n",
        "           leaf_font_size=6)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CbeAGMzQUwni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise\n",
        "# Extracting the cluster labels\n",
        "\n",
        "# Perform the necessary imports\n",
        "import pandas as pd\n",
        "from scipy.cluster.hierarchy import fcluster\n",
        "\n",
        "# Use fcluster to extract labels: labels\n",
        "labels = fcluster(mergings, 6, criterion='distance')\n",
        "\n",
        "# Create a DataFrame with labels and varieties as columns: df\n",
        "df = pd.DataFrame({'labels': labels, 'varieties': varieties})\n",
        "\n",
        "# Create crosstab: ct\n",
        "ct = pd.crosstab(df['labels'], df['varieties'])\n",
        "\n",
        "# Display ct\n",
        "print(ct)\n"
      ],
      "metadata": {
        "id": "AXMiq-5UUwkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pY8NG3roWpii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kRbczbn8WpbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## t-SNE for 2-dimensional maps"
      ],
      "metadata": {
        "id": "FwSWfcbyVGZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# t-SNE in sklearn\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "model = TSNE(learning_rate=100)\n",
        "transformed = model.fit_transform(samples)\n",
        "xs = transformed[:,0]\n",
        "ys = transformed[:,1]\n",
        "plt.scatter(xs, ys, c=species)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Vs52oaBBUwi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Bo0tzaZYKS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise\n",
        "# t-SNE visualization of grain dataset\n",
        "\n",
        "# Import TSNE\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Create a TSNE instance: model\n",
        "model = TSNE(learning_rate=200)\n",
        "\n",
        "# Apply fit_transform to samples: tsne_features\n",
        "tsne_features = model.fit_transform(samples)\n",
        "\n",
        "# Select the 0th feature: xs\n",
        "xs = tsne_features[:,0]\n",
        "\n",
        "# Select the 1st feature: ys\n",
        "ys = tsne_features[:,1]\n",
        "\n",
        "# Scatter plot, coloring by variety_numbers\n",
        "plt.scatter(xs, ys, c=variety_numbers)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lEO442sqYKM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise\n",
        "# A t-SNE map of the stock market\n",
        "\n",
        "# Import TSNE\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Create a TSNE instance: model\n",
        "model = TSNE(learning_rate=50)\n",
        "\n",
        "# Apply fit_transform to normalized_movements: tsne_features\n",
        "tsne_features = model.fit_transform(normalized_movements)\n",
        "\n",
        "# Select the 0th feature: xs\n",
        "xs = tsne_features[:,0]\n",
        "\n",
        "# Select the 1th feature: ys\n",
        "ys = tsne_features[:,1]\n",
        "\n",
        "# Scatter plot\n",
        "plt.scatter(xs, ys, alpha=0.5)\n",
        "\n",
        "# Annotate the points\n",
        "for x, y, company in zip(xs, ys, companies):\n",
        "    plt.annotate(company, (x, y), fontsize=5, alpha=0.75)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tSsfRHthUwg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N1cmGyidUvfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Qwv_bg3Zhq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decorrelating your data and dimension reduction"
      ],
      "metadata": {
        "id": "C3t4ouRnZm2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing the PCA transformation"
      ],
      "metadata": {
        "id": "01DxU4X6Ziqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using scikit-learn PCA\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "model = PCA()\n",
        "model.fit(samples)"
      ],
      "metadata": {
        "id": "3uoLaKRAZho2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformed = model.transform(samples)"
      ],
      "metadata": {
        "id": "RWMXEhyWZhmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise\n",
        "# Correlated data in nature\n",
        "\n",
        "# Perform the necessary imports\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Assign the 0th column of grains: width\n",
        "width = grains[:,0]\n",
        "\n",
        "# Assign the 1st column of grains: length\n",
        "length = grains[:,1]\n",
        "\n",
        "# Scatter plot width vs length\n",
        "plt.scatter(width, length)\n",
        "plt.axis('equal')\n",
        "plt.show()\n",
        "\n",
        "# Calculate the Pearson correlation\n",
        "correlation, pvalue = pearsonr(width, length)\n",
        "\n",
        "# Display the correlation\n",
        "print(correlation)\n"
      ],
      "metadata": {
        "id": "z-uE5YxnZhj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise\n",
        "# Decorrelating the grain measurements with PCA\n",
        "\n",
        "# Import PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Create PCA instance: model\n",
        "model = PCA()\n",
        "\n",
        "# Apply the fit_transform method of model to grains: pca_features\n",
        "pca_features = model.fit_transform(grains)\n",
        "\n",
        "# Assign 0th column of pca_features: xs\n",
        "xs = pca_features[:,0]\n",
        "\n",
        "# Assign 1st column of pca_features: ys\n",
        "ys = pca_features[:,1]\n",
        "\n",
        "# Scatter plot xs vs ys\n",
        "plt.scatter(xs, ys)\n",
        "plt.axis('equal')\n",
        "plt.show()\n",
        "\n",
        "# Calculate the Pearson correlation of xs and ys\n",
        "correlation, pvalue = pearsonr(xs, ys)\n",
        "\n",
        "# Display the correlation\n",
        "print(correlation)"
      ],
      "metadata": {
        "id": "_gOQk2YqZhha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b5m5QH29ZhfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intrinsic dimension"
      ],
      "metadata": {
        "id": "hqQDBApYbZ5B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hNwV6StQcRKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Plotting the variances of PCA features\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA()\n",
        "pca.fit(samples)\n",
        "\n",
        "features = range(pca.n_components_)\n",
        "\n",
        "plt.bar(features, pca.explained_variance_)\n",
        "plt.xticks(features)\n",
        "plt.ylabel('variance')\n",
        "plt.xlabel('PCA feature')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aUb4KLbWZhcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise\n",
        "# The first principal component\n",
        "\n",
        "# Make a scatter plot of the untransformed points\n",
        "plt.scatter(grains[:,0], grains[:,1])\n",
        "\n",
        "# Create a PCA instance: model\n",
        "model = PCA()\n",
        "\n",
        "# Fit model to points\n",
        "model.fit(grains)\n",
        "\n",
        "# Get the mean of the grain samples: mean\n",
        "mean = model.mean_\n",
        "\n",
        "# Get the first principal component: first_pc\n",
        "first_pc = model.components_[0,:]\n",
        "\n",
        "# Plot first_pc as an arrow, starting at mean\n",
        "plt.arrow(mean[0], mean[1], first_pc[0], first_pc[1], color='red', width=0.01)\n",
        "\n",
        "# Keep axes on same scale\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R2fLddTAZhYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise\n",
        "# Variance of the PCA features\n",
        "\n",
        "# Perform the necessary imports\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create scaler: scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Create a PCA instance: pca\n",
        "pca = PCA()\n",
        "\n",
        "# Create pipeline: pipeline\n",
        "pipeline = make_pipeline(scaler, pca)\n",
        "\n",
        "# Fit the pipeline to 'samples'\n",
        "pipeline.fit(samples)\n",
        "\n",
        "# Plot the explained variances\n",
        "features =  range(pca.n_components_)\n",
        "plt.bar(features, pca.explained_variance_)\n",
        "plt.xlabel('PCA feature')\n",
        "plt.ylabel('variance')\n",
        "plt.xticks(features)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "iEZFTJVyZhW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iKVXJ60QZhR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dimension reduction with PCA"
      ],
      "metadata": {
        "id": "esnDlEwocbax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimension reduction of iris dataset\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(samples)\n",
        "\n",
        "transformed = pca.transform(samples)\n",
        "print(transformed.shape)"
      ],
      "metadata": {
        "id": "PyhlhbYwZhBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TruncatedSVD and csr_matrix\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "model = TruncatedSVD(n_components=3)\n",
        "model.fit(documents) # documents is csr_matrix\n",
        "TruncatedSVD(algorithm='randomized', ... )\n",
        "transformed = model.transform(documents)"
      ],
      "metadata": {
        "id": "_C7rDeINcoCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise\n",
        "# Dimension reduction of the fish measurements\n",
        "\n",
        "# Import PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Create a PCA model with 2 components: pca\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "# Fit the PCA instance to the scaled samples\n",
        "pca.fit(scaled_samples)\n",
        "\n",
        "# Transform the scaled samples: pca_features\n",
        "pca_features = pca.transform(scaled_samples)\n",
        "\n",
        "# Print the shape of pca_features\n",
        "print(pca_features.shape)\n"
      ],
      "metadata": {
        "id": "HZcpvpUscn2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise\n",
        "# A tf-idf word-frequency array\n",
        "\n",
        "# Import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Create a TfidfVectorizer: tfidf\n",
        "tfidf = TfidfVectorizer() \n",
        "\n",
        "# Apply fit_transform to document: csr_mat\n",
        "csr_mat = tfidf.fit_transform(documents) \n",
        "\n",
        "# Print result of toarray() method\n",
        "print(csr_mat.toarray())\n",
        "\n",
        "# Get the words: words\n",
        "words = tfidf.get_feature_names()\n",
        "\n",
        "# Print words\n",
        "print(words)\n"
      ],
      "metadata": {
        "id": "L3ka65gIcnz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise\n",
        "# Clustering Wikipedia part I\n",
        "\n",
        "# Perform the necessary imports\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Create a TruncatedSVD instance: svd\n",
        "svd = TruncatedSVD(n_components=50)\n",
        "\n",
        "# Create a KMeans instance: kmeans\n",
        "kmeans = KMeans(n_clusters=6)\n",
        "\n",
        "# Create a pipeline: pipeline\n",
        "pipeline = make_pipeline(svd, kmeans)\n"
      ],
      "metadata": {
        "id": "62u_AesxcnxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise\n",
        "# Clustering Wikipedia part II\n",
        "\n",
        "# Import pandas\n",
        "import pandas as pd \n",
        "\n",
        "# Fit the pipeline to articles\n",
        "pipeline.fit(articles)\n",
        "\n",
        "# Calculate the cluster labels: labels\n",
        "labels = pipeline.predict(articles)\n",
        "\n",
        "# Create a DataFrame aligning labels and titles: df\n",
        "df = pd.DataFrame({'label': labels, 'article': titles})\n",
        "\n",
        "# Display df sorted by cluster label\n",
        "print(df.sort_values('label'))\n",
        "\n"
      ],
      "metadata": {
        "id": "tvVC7EWRcnuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UAHKP1Amcnro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dvNC89Excnng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discovering interpretable features"
      ],
      "metadata": {
        "id": "LEBz7V4acnXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##"
      ],
      "metadata": {
        "id": "HS3DwuhXc2Kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Non-negative matrix factorization (NMF)"
      ],
      "metadata": {
        "id": "Q4atXOGSgYxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage of NMF\n",
        "\n",
        "from sklearn.decomposition import NMF\n",
        "model = NMF(n_components=2)\n",
        "model.fit(samples)\n",
        "\n",
        "\n",
        "nmf_features = model.transform(samples)"
      ],
      "metadata": {
        "id": "X2yX6znbgXRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reconstruction of a sample\n",
        "\n",
        "print(samples[i,:])\n",
        "\n",
        "\n",
        "print(nmf_features[i,:])"
      ],
      "metadata": {
        "id": "wgd7TZulgYEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise\n",
        "# NMF applied to Wikipedia article\n",
        "\n",
        "# Import NMF\n",
        "from sklearn.decomposition import NMF\n",
        "\n",
        "# Create an NMF instance: model\n",
        "model = NMF(n_components=6)\n",
        "\n",
        "# Fit the model to artaicles\n",
        "model.fit(articles)\n",
        "\n",
        "# Transform the articles: nmf_features\n",
        "nmf_features = model.transform(articles)\n",
        "\n",
        "# Print the NMF features\n",
        "print(nmf_features.round(2))\n"
      ],
      "metadata": {
        "id": "ojF2jo8lhMxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise\n",
        "# NMF features of the Wikipedia articles\n",
        "\n",
        "# Import pandas\n",
        "import pandas as pd \n",
        "\n",
        "# Create a pandas DataFrame: df\n",
        "df = pd.DataFrame(nmf_features, index=titles)\n",
        "\n",
        "# Print the row for 'Anne Hathaway'\n",
        "print(df.loc['Anne Hathaway'])\n",
        "\n",
        "# Print the row for 'Denzel Washington'\n",
        "print(df.loc['Denzel Washington'])\n"
      ],
      "metadata": {
        "id": "GzWyihWLiXkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wRCZE1sohMvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h-giesCAhMsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NMF learns interpretable parts"
      ],
      "metadata": {
        "id": "yubaNriqhNPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying NMF to the articles\n",
        "\n",
        "print(articles.shape)\n",
        "\n",
        "from sklearn.decomposition import NMF\n",
        "nmf = NMF(n_components=10)\n",
        "nmf.fit(articles)\n",
        "\n",
        "\n",
        "print(nmf.components_.shape)"
      ],
      "metadata": {
        "id": "mGACMthJhMnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing samples\n",
        "\n",
        "bitmap = sample.reshape((2, 3))\n",
        "print(bitmap)\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "plt.imshow(bitmap, cmap='gray', interpolation='nearest')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6TH6atv9hMko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise\n",
        "# NMF learns topics of documents\n",
        "\n",
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame: components_df\n",
        "components_df = pd.DataFrame(model.components_, columns=words)\n",
        "\n",
        "\n",
        "# Print the shape of the DataFrame\n",
        "print(components_df.shape)\n",
        "\n",
        "# Select row 3: component\n",
        "component = components_df.iloc[3]\n",
        "\n",
        "# Print result of nlargest\n",
        "print(component.nlargest())\n"
      ],
      "metadata": {
        "id": "as-D00-ghMiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise\n",
        "# Explore the LED digits dataset\n",
        "\n",
        "# Import pyplot\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Select the 0th row: digit\n",
        "digit = samples[0,:]\n",
        "\n",
        "# Print digit\n",
        "print(digit)\n",
        "\n",
        "# Reshape digit to a 13x8 array: bitmap\n",
        "bitmap = digit.reshape(13, 8)\n",
        "\n",
        "# Print bitmap\n",
        "print(bitmap)\n",
        "\n",
        "# Use plt.imshow to display bitmap\n",
        "plt.imshow(bitmap, cmap='gray', interpolation='nearest')\n",
        "plt.colorbar()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Cekeyh5RhMW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise\n",
        "# NMF learns the parts of images\n",
        "\n",
        "# Import NMF\n",
        "from sklearn.decomposition import NMF\n",
        "\n",
        "# Create an NMF model: model\n",
        "model = NMF(n_components=7)\n",
        "\n",
        "# Apply fit_transform to samples: features\n",
        "features = model.fit_transform(samples)\n",
        "\n",
        "# Call show_as_image on each component\n",
        "for component in model.components_:\n",
        "    show_as_image(component)\n",
        "\n",
        "# Select the 0th row of features: digit_features\n",
        "digit_features = features[0,:]\n",
        "\n",
        "# Print digit_features\n",
        "print(digit_features)"
      ],
      "metadata": {
        "id": "l5a0N4dEhMUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise\n",
        "# PCA doesn't learn parts\n",
        "\n",
        "# Import PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Create a PCA instance: model\n",
        "model = PCA(n_components=7)\n",
        "\n",
        "# Apply fit_transform to samples: features\n",
        "features = model.fit_transform(samples)\n",
        "\n",
        "# Call show_as_image on each component\n",
        "for component in model.components_:\n",
        "    show_as_image(component)\n",
        "    "
      ],
      "metadata": {
        "id": "9c8FxzTvhMRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building recommender systems using NMF"
      ],
      "metadata": {
        "id": "nfhFAx4vjTZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply NMF to the word-frequency array\n",
        "\n",
        "from sklearn.decomposition import NMF\n",
        "nmf = NMF(n_components=6)\n",
        "nmf_features = nmf.fit_transform(articles)"
      ],
      "metadata": {
        "id": "N9BkeSwjhMO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the cosine similarities\n",
        "\n",
        "from sklearn.preprocessing import normalize\n",
        "norm_features = normalize(nmf_features)\n",
        "# if has index 23\n",
        "current_article = norm_features[23,:]\n",
        "similarities = norm_features.dot(current_article)\n",
        "print(similarities)"
      ],
      "metadata": {
        "id": "m_B_FxUNhMMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataFrames and labels\n",
        "\n",
        "import pandas as pd\n",
        "norm_features = normalize(nmf_features)\n",
        "df = pd.DataFrame(norm_features, index=titles)\n",
        "current_article = df.loc['Dog bites man']\n",
        "similarities = df.dot(current_article)\n",
        "\n",
        "print(similarities.nlargest())"
      ],
      "metadata": {
        "id": "jcN6jt9EhMJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise\n",
        "# Which articles are similar to 'Cristiano Ronaldo'?\n",
        "\n",
        "# Perform the necessary imports\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "# Normalize the NMF features: norm_features\n",
        "norm_features = normalize(nmf_features)\n",
        "\n",
        "# Create a DataFrame: df\n",
        "df = pd.DataFrame(norm_features, index=titles)\n",
        "\n",
        "# Select the row corresponding to 'Cristiano Ronaldo': article\n",
        "article = df.loc['Cristiano Ronaldo']\n",
        "\n",
        "# Compute the dot products: similarities\n",
        "similarities = df.dot(article)\n",
        "\n",
        "# Display those with the largest cosine similarity\n",
        "print(similarities.nlargest())\n"
      ],
      "metadata": {
        "id": "UIffNZPWhMHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise\n",
        "# Recommend musical artists part I\n",
        "\n",
        "# Perform the necessary imports\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.preprocessing import Normalizer, MaxAbsScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Create a MaxAbsScaler: scaler\n",
        "scaler = MaxAbsScaler()\n",
        "\n",
        "# Create an NMF model: nmf\n",
        "nmf = NMF(n_components = 20)\n",
        "\n",
        "# Create a Normalizer: normalizer\n",
        "normalizer = Normalizer()\n",
        "\n",
        "# Create a pipeline: pipeline\n",
        "pipeline = make_pipeline(scaler, nmf, normalizer)\n",
        "\n",
        "# Apply fit_transform to artists: norm_features\n",
        "norm_features = pipeline.fit_transform(artists)\n"
      ],
      "metadata": {
        "id": "dbQTlk3NhMEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise\n",
        "# Recommend musical artists part II\n",
        "\n",
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame: df\n",
        "df = pd.DataFrame(norm_features, index = artist_names)\n",
        "\n",
        "# Select row of 'Bruce Springsteen': artist\n",
        "artist = df.loc['Bruce Springsteen']\n",
        "\n",
        "# Compute cosine similarities: similarities\n",
        "similarities = df.dot(artist)\n",
        "\n",
        "# Display those with highest cosine similarity\n",
        "print(similarities.nlargest())\n",
        "\n"
      ],
      "metadata": {
        "id": "Bjudb8OchMBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Pj9lAi0hL_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QVVtREhnhL8S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}